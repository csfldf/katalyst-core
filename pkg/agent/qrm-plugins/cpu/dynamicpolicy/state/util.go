/*
Copyright 2022 The Katalyst Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package state

import (
	"fmt"

	"k8s.io/apimachinery/pkg/util/errors"
	"k8s.io/apimachinery/pkg/util/sets"
	"k8s.io/klog/v2"

	"github.com/kubewharf/katalyst-api/pkg/consts"
	advisorapi "github.com/kubewharf/katalyst-core/pkg/agent/qrm-plugins/cpu/dynamicpolicy/cpuadvisor"
	"github.com/kubewharf/katalyst-core/pkg/util/general"
	"github.com/kubewharf/katalyst-core/pkg/util/machine"
)

type ListAndWatchResponseValidator func(entries PodEntries, resp *advisorapi.ListAndWatchResponse) error

const (
	PoolNameShare     = "share"
	PoolNameReclaim   = "reclaim"
	PoolNameDedicated = "dedicated"
	PoolNameReserve   = "reserve"
	// PoolNameFallback is not a real pool and is the union result of
	// all online pools to put pod should have been isolated
	PoolNameFallback = "fallback"
)

var (
	// StaticPools are generated by cpu plugin statically,
	// and they will be ignored when reading cpu advisor list and watch response
	StaticPools = sets.NewString(
		PoolNameReserve,
	)

	// ResidentPools are guaranteed existing in state
	ResidentPools = sets.NewString(
		PoolNameReclaim,
	).Union(StaticPools)
)

var GetContainerRequestedCores func(allocationInfo *AllocationInfo) int

// [TODO]: valida all shared_cores and reclaimed_cores haven't numa results
var listAndWatchResponseValidators = []ListAndWatchResponseValidator{
	validateStaticPools,
	validateDedicatedEntries,
}

func GetIsolatedQuantityMapFromPodEntries(podEntries PodEntries, ignoreAllocationInfos []*AllocationInfo) map[string]map[string]int {
	ret := make(map[string]map[string]int)

	for podUID, entries := range podEntries {
		if entries.IsPoolEntry() {
			continue
		}
	containerLoop:
		for containerName, allocationInfo := range entries {
			// only filter dedicated_cores without numa_binding
			if allocationInfo == nil ||
				// dedicated_cores with numa_binding
				(allocationInfo.QoSLevel == consts.PodAnnotationQoSLevelDedicatedCores &&
					allocationInfo.Annotations[consts.PodAnnotationMemoryEnhancementNumaBinding] == consts.PodAnnotationMemoryEnhancementNumaBindingEnable) ||
				(allocationInfo.QoSLevel != consts.PodAnnotationQoSLevelDedicatedCores) {
				continue
			}

			for _, ignoreAllocationInfo := range ignoreAllocationInfos {
				if allocationInfo.PodUid == ignoreAllocationInfo.PodUid && allocationInfo.ContainerName == ignoreAllocationInfo.ContainerName {
					continue containerLoop
				}
			}

			// if there is no more cores to allocate, we will put dedicated_cores without numa_binding
			// to pool rather than isolation. calling this function means we will start to adjust allocation,
			// and we will try to isolate those containers, so we will treat them as containers to be isolated.
			var quantity int
			if allocationInfo.OwnerPoolName != PoolNameDedicated {
				quantity = GetContainerRequestedCores(allocationInfo)
			} else {
				quantity = allocationInfo.AllocationResult.Size()
			}

			if quantity == 0 {
				klog.Warningf("[GetIsolatedQuantityMapFromPodEntries] isolated pod: %s/%s container: %s get zero quantity",
					allocationInfo.PodNamespace, allocationInfo.PodName, allocationInfo.ContainerName)
				continue
			}

			if ret[podUID] == nil {
				ret[podUID] = make(map[string]int)
			}
			ret[podUID][containerName] = quantity
		}
	}

	return ret
}

// GetPoolsQuantityMapFromPodEntries is used to constrict mapping from
// pool name to its corresponding resource quantity
func GetPoolsQuantityMapFromPodEntries(podEntries PodEntries, ignoreAllocationInfos []*AllocationInfo) map[string]int {
	ret := make(map[string]int)

	for _, entries := range podEntries {
		if entries.IsPoolEntry() {
			continue
		}
	containerLoop:
		for _, allocationInfo := range entries {
			// only count shared_cores not isolated.
			// if there is no more cores to allocate, we will put dedicated_cores without numa_binding to pool rather than isolation.
			// calling this function means we will start to adjust allocation and we will try to isolate those containers,
			// so we will treat them as containers to be isolated.
			if allocationInfo == nil || allocationInfo.QoSLevel != consts.PodAnnotationQoSLevelSharedCores {
				continue
			}

			for _, ignoreAllocationInfo := range ignoreAllocationInfos {
				if allocationInfo.PodUid == ignoreAllocationInfo.PodUid && allocationInfo.ContainerName == ignoreAllocationInfo.ContainerName {
					continue containerLoop
				}
			}

			poolName := GetRealOwnerPoolName(allocationInfo)
			if poolName != "" {
				ret[poolName] += GetContainerRequestedCores(allocationInfo)
			}
		}
	}

	return ret
}

func GetMainContainerPoolName(containerEntries ContainerEntries) string {
	return GetRealOwnerPoolName(containerEntries.GetMainContainerEntry())
}

// GetSpecifiedPoolName parses the belonging pool name for a given allocation according to QoS level
func GetSpecifiedPoolName(allocationInfo *AllocationInfo) string {
	if allocationInfo == nil {
		return ""
	}

	switch allocationInfo.QoSLevel {
	case consts.PodAnnotationQoSLevelSharedCores:
		specifiedPoolName := GetSpecifiedPoolNameForSharedCores(allocationInfo)
		if specifiedPoolName != "" {
			return specifiedPoolName
		}
		return PoolNameShare
	case consts.PodAnnotationQoSLevelReclaimedCores:
		return PoolNameReclaim
	case consts.PodAnnotationQoSLevelDedicatedCores:
		return PoolNameDedicated
	default:
		return ""
	}
}

// GetRealOwnerPoolName parses the owner pool name for a given allocation
func GetRealOwnerPoolName(allocationInfo *AllocationInfo) string {
	if allocationInfo == nil {
		return ""
	}

	return allocationInfo.OwnerPoolName
}

// GetSpecifiedPoolNameForSharedCores returns cpu enhancement from allocation
func GetSpecifiedPoolNameForSharedCores(allocationInfo *AllocationInfo) string {
	if allocationInfo == nil {
		return ""
	}

	return allocationInfo.Annotations[consts.PodAnnotationCPUEnhancementCPUSet]
}

// GenerateCPUMachineStateByPodEntries is used to re-organize podEntries
// by assembling allocation info into each NUMA node
func GenerateCPUMachineStateByPodEntries(topology *machine.CPUTopology, podEntries PodEntries) (NUMANodeMap, error) {
	if topology == nil {
		return nil, fmt.Errorf("GenerateCPUMachineStateByPodEntries got nil topology")
	}

	machineState := make(NUMANodeMap)
	for _, numaNode := range topology.CPUDetails.NUMANodes().ToSliceInt64() {
		numaNodeState := &NUMANodeState{}

		numaNodeAllCPUs := topology.CPUDetails.CPUsInNUMANodes(int(numaNode)).Clone()
		allocatedCPUsInNumaNode := machine.NewCPUSet()

		for podUID, containerEntries := range podEntries {
			for containerName, allocationInfo := range containerEntries {
				if containerName != "" && allocationInfo != nil {

					// the container hasn't cpuset assignment in the current NUMA node
					if allocationInfo.OriginalTopologyAwareAssignments[int(numaNode)].Size() == 0 &&
						allocationInfo.TopologyAwareAssignments[int(numaNode)].Size() == 0 {
						continue
					}

					// only modify allocated and default properties in NUMA node state for dedicated_cores with NUMA binding
					if allocationInfo.QoSLevel == consts.PodAnnotationQoSLevelDedicatedCores &&
						allocationInfo.Annotations[consts.PodAnnotationMemoryEnhancementNumaBinding] == consts.PodAnnotationMemoryEnhancementNumaBindingEnable {
						// only consider original in machine state
						allocatedCPUsInNumaNode = allocatedCPUsInNumaNode.Union(allocationInfo.OriginalTopologyAwareAssignments[int(numaNode)])
					}

					numaNodeAllocationInfo := allocationInfo.Clone()

					topologyAwareAssignments, _ := machine.GetNumaAwareAssignments(topology, allocationInfo.AllocationResult.Intersection(numaNodeAllCPUs))
					originalTopologyAwareAssignments, _ := machine.GetNumaAwareAssignments(topology, allocationInfo.OriginalAllocationResult.Intersection(numaNodeAllCPUs))

					numaNodeAllocationInfo.AllocationResult = allocationInfo.AllocationResult.Intersection(numaNodeAllCPUs)
					numaNodeAllocationInfo.OriginalAllocationResult = allocationInfo.OriginalAllocationResult.Intersection(numaNodeAllCPUs)
					numaNodeAllocationInfo.TopologyAwareAssignments = topologyAwareAssignments
					numaNodeAllocationInfo.OriginalTopologyAwareAssignments = originalTopologyAwareAssignments

					numaNodeState.SetAllocationInfo(podUID, containerName, numaNodeAllocationInfo)
				}
			}
		}

		numaNodeState.AllocatedCPUSet = allocatedCPUsInNumaNode.Clone()
		numaNodeState.DefaultCPUSet = numaNodeAllCPUs.Difference(numaNodeState.AllocatedCPUSet)

		machineState[int(numaNode)] = numaNodeState
	}

	return machineState, nil
}

func validateDedicatedEntries(entries PodEntries, resp *advisorapi.ListAndWatchResponse) error {
	dedicatedAllocationInfos := FilterDedicatedAllocationInfos(entries)
	dedicatedCalculationInfos := FilterDedicatedCalculationInfos(resp)

	if len(dedicatedAllocationInfos) != len(dedicatedCalculationInfos) {
		return fmt.Errorf("dedicatedAllocationInfos length: %d and dedicatedCalculationInfos length: %d mismatch",
			len(dedicatedAllocationInfos), len(dedicatedCalculationInfos))
	}

	for podUID, containerEntries := range dedicatedAllocationInfos {
		for containerName, allocationInfo := range containerEntries {

			calculationInfo := dedicatedCalculationInfos[podUID][containerName]

			if calculationInfo == nil {
				return fmt.Errorf("missing CalculationInfo for pod: %s container: %s", podUID, containerName)
			}

			if allocationInfo.Annotations[consts.PodAnnotationMemoryEnhancementNumaBinding] == consts.PodAnnotationMemoryEnhancementNumaBindingEnable {
				numaCalculationQuantities, err := GetCalculationInfoNUMAQuantities(calculationInfo)

				if err != nil {
					return fmt.Errorf("GetCalculationInfoNUMAQuantities failed with error: %v, pod: %s container: %s",
						err, podUID, containerName)
				}

				// currently we don't support strategy to adjust cpuset of dedicated_cores containers.
				// for stability if the dedicated_cores container calculation result and allocation result,
				// we will return error.

				for numaId, cset := range allocationInfo.TopologyAwareAssignments {
					if cset.Size() != numaCalculationQuantities[numaId] {
						return fmt.Errorf("NUMA: %d calculation quantity: %d and allocation quantity: %d mismatch, pod: %s container: %s",
							numaId, numaCalculationQuantities[numaId], cset.Size(), podUID, containerName)
					}
				}

				for numaId, calQuantity := range numaCalculationQuantities {
					if calQuantity != allocationInfo.TopologyAwareAssignments[numaId].Size() {
						return fmt.Errorf("NUMA: %d calculation quantity: %d and allocation quantity: %d mismatch, pod: %s container: %s",
							numaId, calQuantity, allocationInfo.TopologyAwareAssignments[numaId].Size(), podUID, containerName)
					}
				}
			} else {
				calculationQuantity, err := GetCalculationInfoTotalQuantity(calculationInfo)

				if err != nil {
					return fmt.Errorf("GetCalculationInfoTotalQuantity failed with error: %v, pod: %s container: %s",
						err, podUID, containerName)
				}

				// currently we don't support strategy to adjust cpuset of dedicated_cores containers.
				// for stability if the dedicated_cores container calculation result and allocation result,
				// we will return error.
				if calculationQuantity != allocationInfo.AllocationResult.Size() {
					return fmt.Errorf("pod: %s container: %s calculation result: %d and allocation result: %d mismatch",
						podUID, containerName, calculationQuantity, allocationInfo.AllocationResult.Size())
				}
			}
		}
	}

	return nil
}

func validateStaticPools(entries PodEntries, resp *advisorapi.ListAndWatchResponse) error {
	for _, poolName := range StaticPools.List() {

		var nilStateEntry, nilRespEntry bool
		if entries[poolName] == nil || entries[poolName][""] == nil {
			nilStateEntry = true
		}

		if resp.Entries[poolName] == nil || resp.Entries[poolName].Entries[""] == nil {
			nilRespEntry = true
		}

		if nilStateEntry != nilRespEntry {
			return fmt.Errorf("pool: %s nilStateEntry: %v and nilRespEntry: %v mismatch",
				poolName, nilStateEntry, nilStateEntry)
		}

		if nilStateEntry {
			klog.Warningf("[validateStaticPools] got nil state entry for static pool: %s", poolName)
			continue
		}

		allocationInfo := entries[poolName][""]
		calculationInfo := resp.Entries[poolName].Entries[""]

		if calculationInfo.OwnerPoolName != poolName {
			return fmt.Errorf("pool: %s has invalid owner pool name: %s in cpu advisor resp",
				poolName, calculationInfo.OwnerPoolName)
		}

		if len(calculationInfo.CalculationResultsByNumas) != 1 ||
			calculationInfo.CalculationResultsByNumas[-1] == nil ||
			len(calculationInfo.CalculationResultsByNumas[-1].Blocks) != 1 {
			return fmt.Errorf("static pool: %s has invalid calculationInfo", poolName)
		}

		calculationQuantity, err := GetCalculationInfoTotalQuantity(calculationInfo)

		if err != nil {
			return fmt.Errorf("GetCalculationInfoTotalQuantity failed with error: %v, pool: %s",
				err, poolName)
		}

		// currently we don't support strategy to adjust cpuset of static pools.
		// for stability if the static pool calculation result and allocation result,
		// we will return error.
		if calculationQuantity != allocationInfo.AllocationResult.Size() {
			return fmt.Errorf("static pool: %s calculation result: %d and allocation result: %d mismatch",
				poolName, calculationQuantity, allocationInfo.AllocationResult.Size())
		}
	}

	return nil
}

func ValidateCPUAdvisorResp(entries PodEntries, resp *advisorapi.ListAndWatchResponse) error {

	if resp == nil {
		return fmt.Errorf("got nil cpu advisor resp")
	}

	var errList []error

	for _, validator := range listAndWatchResponseValidators {
		errList = append(errList, validator(entries, resp))
	}

	return errors.NewAggregate(errList)
}

func FilterDedicatedCalculationInfos(resp *advisorapi.ListAndWatchResponse) map[string]map[string]*advisorapi.CalculationInfo {
	dedicatedCalculationInfos := make(map[string]map[string]*advisorapi.CalculationInfo)

	for entryName, entry := range resp.Entries {
		for subEntryName, calculationInfo := range entry.Entries {
			if calculationInfo != nil && calculationInfo.OwnerPoolName == PoolNameDedicated {

				if dedicatedCalculationInfos[entryName] == nil {
					dedicatedCalculationInfos[entryName] = make(map[string]*advisorapi.CalculationInfo)
				}

				dedicatedCalculationInfos[entryName][subEntryName] = calculationInfo
			}
		}
	}

	return dedicatedCalculationInfos
}

func FilterDedicatedAllocationInfos(entries PodEntries) PodEntries {
	numaBindingEntries := make(PodEntries)

	for podUID, containerEntries := range entries {
		if containerEntries.IsPoolEntry() {
			continue
		}

		for containerName, allocationInfo := range containerEntries {
			if allocationInfo != nil &&
				allocationInfo.QoSLevel == consts.PodAnnotationQoSLevelDedicatedCores {

				if numaBindingEntries[podUID] == nil {
					numaBindingEntries[podUID] = make(ContainerEntries)
				}

				numaBindingEntries[podUID][containerName] = allocationInfo.Clone()
			}
		}
	}

	return numaBindingEntries
}

func GetCalculationInfoNUMAQuantities(calculationInfo *advisorapi.CalculationInfo) (map[int]int, error) {
	if calculationInfo == nil {
		return nil, fmt.Errorf("got nil calculationInfo")
	}

	numaQuantities := make(map[int]int)
	for numaId, numaResult := range calculationInfo.CalculationResultsByNumas {
		if numaResult == nil {
			klog.Warningf("[GetCalculationInfoTotalQuantity] got nil NUMA result")
			continue
		}

		var quantityUInt64 uint64 = 0
		for _, block := range numaResult.Blocks {
			if block == nil {
				klog.Warningf("[GetCalculationInfoTotalQuantity] got nil block")
				continue
			}

			quantityUInt64 += block.Result
		}

		quantityInt, err := general.CovertUInt64ToInt(quantityUInt64)

		if err != nil {
			return nil, fmt.Errorf("converting quantity: %d to int failed with error: %v",
				quantityUInt64, err)
		}

		numaIdInt, err := general.CovertInt64ToInt(numaId)

		if err != nil {
			return nil, fmt.Errorf("converting quantity: %d to int failed with error: %v",
				numaId, err)
		}

		numaQuantities[numaIdInt] = quantityInt
	}

	return numaQuantities, nil
}

func GetCalculationInfoTotalQuantity(calculationInfo *advisorapi.CalculationInfo) (int, error) {
	if calculationInfo == nil {
		return 0, fmt.Errorf("got nil calculationInfo")
	}

	var quantityUInt64 uint64 = 0
	for _, numaResult := range calculationInfo.CalculationResultsByNumas {
		if numaResult == nil {
			klog.Warningf("[GetCalculationInfoTotalQuantity] got nil NUMA result")
			continue
		}

		for _, block := range numaResult.Blocks {
			if block == nil {
				klog.Warningf("[GetCalculationInfoTotalQuantity] got nil block")
				continue
			}

			quantityUInt64 += block.Result
		}
	}

	quantityInt, err := general.CovertUInt64ToInt(quantityUInt64)

	if err != nil {
		return 0, fmt.Errorf("converting quantity: %d to int failed with error: %v",
			quantityUInt64, err)
	}

	return quantityInt, nil
}
